@misc{austinProgramSynthesisLarge2021,
  title = {Program {{Synthesis}} with {{Large Language Models}}},
  author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.07732},
  note = {Comment: Jacob and Augustus contributed equally}
}

@inproceedings{austinStructuredDenoisingDiffusion2021,
  title = {Structured {{Denoising Diffusion Models}} in {{Discrete State-Spaces}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and {van den Berg}, Rianne},
  year = {2021},
  volume = {34},
  pages = {17981--17993},
  publisher = {Curran Associates, Inc.}
}

@misc{babaProverAgentAgentbased2025,
  title = {Prover {{Agent}}: {{An Agent-based Framework}} for {{Formal Mathematical Proofs}}},
  author = {Baba, Kaito and Liu, Chaoran and Kurita, Shuhei and Sannai, Akiyoshi},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.19923},
  note = {Comment: 22 pages, 2 figures}
}

@misc{berglundReversalCurseLLMs2024,
  title = {The {{Reversal Curse}}: {{LLMs}} Trained on "{{A}} Is {{B}}" Fail to Learn "{{B}} Is {{A}}"},
  author = {Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.12288},
  note = {Comment: 21 pages, 11 figures}
}

@misc{campbellGenerativeFlowsDiscrete2024a,
  title = {Generative {{Flows}} on {{Discrete State-Spaces}}: {{Enabling Multimodal Flows}} with {{Applications}} to {{Protein Co-Design}}},
  author = {Campbell, Andrew and Yim, Jason and Barzilay, Regina and Rainforth, Tom and Jaakkola, Tommi},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.04997},
  note = {Comment: 60 pages, 11 figures, 6 tables; ICML 2024}
}

@misc{gatDiscreteFlowMatching2024a,
  title = {Discrete {{Flow Matching}}},
  author = {Gat, Itai and Remez, Tal and Shaul, Neta and Kreuk, Felix and Chen, Ricky T. Q. and Synnaeve, Gabriel and Adi, Yossi and Lipman, Yaron},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.15595}
}

@misc{gongDiffuCoderUnderstandingImproving2025,
  title = {{{DiffuCoder}}: {{Understanding}} and {{Improving Masked Diffusion Models}} for {{Code Generation}}},
  author = {Gong, Shansan and Zhang, Ruixiang and Zheng, Huangjie and Gu, Jiatao and Jaitly, Navdeep and Kong, Lingpeng and Zhang, Yizhe},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.20639},
  note = {Comment: minor update}
}

@misc{joshiTransformersAreGraph2025,
  title = {Transformers Are {{Graph Neural Networks}}},
  author = {Joshi, Chaitanya K.},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.22084},
  note = {Comment: This paper is a technical version of an article in The Gradient at https://thegradient.pub/transformers-are-graph-neural-networks/}
}

@inproceedings{louDiscreteDiffusionModeling2024,
  title = {Discrete {{Diffusion Modeling}} by {{Estimating}} the {{Ratios}} of the {{Data Distribution}}},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Machine Learning}}},
  author = {Lou, Aaron and Lim, Chenlin and Katariya, Isay and Ermon, Stefano},
  year = {2024},
  series = {{{PMLR}}},
  volume = {235},
  pages = {32103--32138},
  publisher = {{PMLR}},
  address = {{Vienna, Austria}},
  abstract = {Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, modeled by estimating the ratios of the data distribution. We test our method on a suite of tasks including ordinal regression, pixel-level image modeling, and natural language generation. Across all tasks, we find improvements over existing discrete diffusion models and achieve new state-of-the-art results on several benchmarks.},
  url = {https://proceedings.mlr.press/v235/lou24a.html}
}


@misc{nieLargeLanguageDiffusion2025,
  title = {Large {{Language Diffusion Models}}},
  author = {Nie, Shen and Zhu, Fengqi and You, Zebin and Zhang, Xiaolu and Ou, Jingyang and Hu, Jun and Zhou, Jun and Lin, Yankai and Wen, Ji-Rong and Li, Chongxuan},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.09992}
}

@misc{oordNeuralDiscreteRepresentation2018,
  title = {Neural {{Discrete Representation Learning}}},
  author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.00937}
}

@misc{ouYourAbsorbingDiscrete2025,
  title = {Your {{Absorbing Discrete Diffusion Secretly Models}} the {{Conditional Distributions}} of {{Clean Data}}},
  author = {Ou, Jingyang and Nie, Shen and Xue, Kaiwen and Zhu, Fengqi and Sun, Jiacheng and Li, Zhenguo and Li, Chongxuan},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.03736}
}

@misc{sannaiUniversalApproximationsPermutation2019,
  title = {Universal Approximations of Permutation Invariant/Equivariant Functions by Deep Neural Networks},
  author = {Sannai, Akiyoshi and Takai, Yuuki and Cordonnier, Matthieu},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.01939}
}
